{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f216e518",
   "metadata": {},
   "source": [
    "# Sai Pavan Kumar M\n",
    "# Data Science - Batch January 2024 (Hyderabad) - Assignment 12\n",
    "\n",
    "# EDA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828759d2-1be1-4140-b4c8-062d73520474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ppscore as pps\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d3b9df-3624-49e5-95f4-62c113b8ec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to visualise all the columns in the dataframe\n",
    "pd.pandas.set_option('display.max_columns', None)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('adult_with_headers.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c547d826-2490-4829-8026-2d87e86b05e0",
   "metadata": {},
   "source": [
    "## Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161cd618-da11-4522-8118-d1b395e0e932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a3c530-98a0-4caa-a651-f25dcc4b7848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a88c5bc-d4d3-4699-b848-716678527c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f88877f-b34b-49f2-81ee-135ad4da2147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numerical columns\n",
    "numerical_columns = ['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450bc7b7-77c9-4713-a327-21bb551c2599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Scaling\n",
    "standard_scaler = StandardScaler()\n",
    "df_standard_scaled = df.copy()\n",
    "df_standard_scaled[numerical_columns] = standard_scaler.fit_transform(df[numerical_columns])\n",
    "df_standard_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40366c6-5062-4611-ab74-2817e1a31f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max Scaling\n",
    "minmax_scaler = MinMaxScaler()\n",
    "df_minmax_scaled = df.copy()\n",
    "df_minmax_scaled[numerical_columns] = minmax_scaler.fit_transform(df[numerical_columns])\n",
    "df_minmax_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7a2b18-4d90-49d0-8c5c-9ebeffb1283b",
   "metadata": {},
   "source": [
    "#### Discuss the scenarios where each scaling technique is preferred and why\n",
    "- Standard Scaling (StandardScaler): This technique transforms the data to have a mean of 0 and a standard deviation of 1. It is preferred when the data follows a normal distribution or when the model assumes that the data is normally distributed (e.g., linear regression, logistic regression).\n",
    "\n",
    "- Min-Max Scaling (MinMaxScaler): This technique scales the data to a fixed range, usually [0, 1]. It is preferred when the data does not necessarily follow a normal distribution and you want to preserve the relationships of the original data. It is also useful when using algorithms that do not assume any specific distribution of the data, such as k-nearest neighbors and neural networks.s.\n",
    "ks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7233fcd6-713b-4126-9e24-e86c22cdc12c",
   "metadata": {},
   "source": [
    "## Encoding Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4b67e2-bff3-4fd4-b2df-b7c5c1602a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b57340-4af4-435f-b1c2-34c22a1e0b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply One-hot encoding to categorial variables with kess then 5 categories\n",
    "one_hot_columns = [col for col in categorical_columns if df[col].nunique() < 5]\n",
    "df_one_hot_encoded  = pd.get_dummies(df, columns=one_hot_columns, drop_first=True)\n",
    "df_one_hot_encoded .head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deaca32-dc87-469d-a62d-b6af39c02578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Apply Label Encoding to categorical variables with more than 5 categories\n",
    "label_encode_columns = [col for col in categorical_columns if df[col].nunique() >= 5]\n",
    "\n",
    "# Initialize Label Encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply Label Encoding\n",
    "for col in label_encode_columns:\n",
    "    df[col] = label_encoder.fit_transform(df[col])\n",
    "\n",
    "# Display the first few rows of the encoded dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc87f9cf-7eb6-40dd-871b-2c472c9edc38",
   "metadata": {},
   "source": [
    "#### Discuss the pros and cons of One-Hot Encoding and Label Encoding.\n",
    "##### One-Hot Encoding\n",
    "- Pros:\n",
    "    - No Ordinal Relationship Assumption:\n",
    "        - One-Hot Encoding does not assume any ordinal relationship between the categories.         - It treats each category as an independent entity.\n",
    "        - Suitable for nominal categorical variables (e.g., color, gender).\n",
    "    - Avoids Arbitrary Ranking:\n",
    "        - Prevents assigning arbitrary ranking to categories, which could mislead some algorithms (e.g., linear regression).\n",
    "- Cons:\n",
    "    - Increased Dimensionality:\n",
    "        - Increases the number of features, especially for categorical variables with many categories. This can lead to the curse of dimensionality, making the model more complex and computationally expensive.\n",
    "    - Sparse Matrix:\n",
    "        - Results in a sparse matrix, where many values are zeros, which can consume more memory and slow down the computation.\n",
    "\n",
    "##### Label Encoding\n",
    "- Pros:\n",
    "    - Simplicity:\n",
    "        - Simple and straightforward to implement.\n",
    "        - Converts categories to integers, which can be easily interpreted by most algorithms.\n",
    "    - No Increased Dimensionality:\n",
    "        - Does not increase the dimensionality of the dataset, keeping it compact.\n",
    "- Cons:\n",
    "    - Assumes Ordinal Relationship:\n",
    "        - Assumes an ordinal relationship between the categories, which may not be true for nominal categorical variables. This can mislead algorithms into interpreting these numerical values as having some sort of ranking or order.\n",
    "    - Potential Bias:\n",
    "        - Some algorithms might interpret the encoded integers as having inherent ordinal importance, which can introduce bias and affect the modelâ€™s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326413f3-9347-4cbf-9310-c1f5fbb317d3",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a598f1-df0b-412a-868f-8bf783552d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature: 'age_group'\n",
    "# Age groups: 0-18 (Child), 19-35 (Young Adult), 36-60 (Adult), 61+ (Senior)\n",
    "df_one_hot_encoded['age_group'] = pd.cut(df['age'], bins=[0, 18, 35, 60, 100], labels=['Child', 'Young Adult', 'Adult', 'Senior'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519dedcf-3256-4e89-9117-6dc5ae771b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature: 'capital_diff' (difference between capital gain and capital loss)\n",
    "df_one_hot_encoded['capital_diff'] = df['capital_gain'] - df['capital_loss']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce83d84d-fc33-48b7-961c-28d9a96088f7",
   "metadata": {},
   "source": [
    "# Rationale:\n",
    "# - 'age_group': Categorizing age into groups may help the model to better understand the distribution of ages and their relationship with income.\n",
    "# - 'capital_diff': The net capital gain or loss could be a significant indicator of wealth, which may correlate with income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64615b2c-4699-43e3-96e3-b98d8a915e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation to 'capital_gain' due to its high skewness\n",
    "df_one_hot_encoded['log_capital_gain'] = np.log1p(df['capital_gain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591256eb-4843-42ef-a456-b4a47e5746d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check skewness before and after transformation\n",
    "print(\"Skewness of 'capital_gain' before transformation:\", df['capital_gain'].skew())\n",
    "print(\"Skewness of 'log_capital_gain' after transformation:\", df_one_hot_encoded['log_capital_gain'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf91101-dc99-43ca-b397-3f3b4b95e76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the modified dataframe\n",
    "df_one_hot_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbdc2e7-bd4b-44fb-807c-c0ef14efce01",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9df3e2-4bdd-4464-9299-9526df25eab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Isolation Forest model\n",
    "clf = IsolationForest(random_state=42, contamination=0.01)  # Contamination is the proportion of outliers\n",
    "\n",
    "# Fit the Isolation Forest model to numerical features\n",
    "clf.fit(df[numerical_columns])\n",
    "\n",
    "# Predict outliers\n",
    "outliers = clf.predict(df[numerical_columns])\n",
    "\n",
    "# Remove outliers\n",
    "df_cleaned = df[outliers != -1]\n",
    "\n",
    "# Print the shape of the cleaned dataset\n",
    "print(\"Original dataset shape:\", df.shape)\n",
    "print(\"Cleaned dataset shape:\", df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8800c9f-6911-4de6-8882-e96b2c6cad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Pearson correlation coefficient manually\n",
    "correlation_matrix = df_cleaned[numerical_columns].corr()\n",
    "\n",
    "# Square the absolute values of the correlation coefficients to get the PPS\n",
    "pps_matrix = correlation_matrix.applymap(lambda x: np.square(abs(x)))\n",
    "\n",
    "print(pps_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
