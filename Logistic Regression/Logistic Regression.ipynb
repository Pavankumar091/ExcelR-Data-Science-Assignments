{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a191738",
   "metadata": {},
   "source": [
    "# Sai Pavan Kumar M\n",
    "# Data Science - Batch January 2024 (Hyderabad) - Assignment 7\n",
    "\n",
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0f7fb6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (C:\\Users\\PAVAN\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder, StandardScaler\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\__init__.py:52\u001b[0m\n\u001b[0;32m     48\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     53\u001b[0m         combine,\n\u001b[0;32m     54\u001b[0m         ensemble,\n\u001b[0;32m     55\u001b[0m         exceptions,\n\u001b[0;32m     56\u001b[0m         metrics,\n\u001b[0;32m     57\u001b[0m         over_sampling,\n\u001b[0;32m     58\u001b[0m         pipeline,\n\u001b[0;32m     59\u001b[0m         tensorflow,\n\u001b[0;32m     60\u001b[0m         under_sampling,\n\u001b[0;32m     61\u001b[0m         utils,\n\u001b[0;32m     62\u001b[0m     )\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\combine\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The :mod:`imblearn.combine` provides methods which combine\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mover-sampling and under-sampling.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_enn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_tomek\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTEENN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTETomek\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\combine\\_smote_enn.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseOverSampler\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\base.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_classification_targets\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_sampling_strategy, check_target_type\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArraysTransformer\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSamplerMixin\u001b[39;00m(BaseEstimator, metaclass\u001b[38;5;241m=\u001b[39mABCMeta):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\utils\\_param_validation.py:908\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_valid_param  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m    907\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m--> 908\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    909\u001b[0m     HasMethods,\n\u001b[0;32m    910\u001b[0m     Hidden,\n\u001b[0;32m    911\u001b[0m     Interval,\n\u001b[0;32m    912\u001b[0m     Options,\n\u001b[0;32m    913\u001b[0m     StrOptions,\n\u001b[0;32m    914\u001b[0m     _ArrayLikes,\n\u001b[0;32m    915\u001b[0m     _Booleans,\n\u001b[0;32m    916\u001b[0m     _Callables,\n\u001b[0;32m    917\u001b[0m     _CVObjects,\n\u001b[0;32m    918\u001b[0m     _InstancesOf,\n\u001b[0;32m    919\u001b[0m     _IterablesNotString,\n\u001b[0;32m    920\u001b[0m     _MissingValues,\n\u001b[0;32m    921\u001b[0m     _NoneConstraint,\n\u001b[0;32m    922\u001b[0m     _PandasNAConstraint,\n\u001b[0;32m    923\u001b[0m     _RandomStates,\n\u001b[0;32m    924\u001b[0m     _SparseMatrices,\n\u001b[0;32m    925\u001b[0m     _VerboseHelper,\n\u001b[0;32m    926\u001b[0m     make_constraint,\n\u001b[0;32m    927\u001b[0m     validate_params,\n\u001b[0;32m    928\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (C:\\Users\\PAVAN\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py)"
     ]
    }
   ],
   "source": [
    "# Load necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "s\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, confusion_matrix, classification_report, precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1d8dc9",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f43b99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Titanic_train.csv\n",
    "df_train = pd.read_csv('Titanic_train.csv')\n",
    "df_train.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601b3ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total no.of rows and columns\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21698d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop serier which aren't necessary\n",
    "df_train = df_train.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19ed823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the training set\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c873dfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "df_train['Age'].fillna(df_train['Age'].median(), inplace=True)\n",
    "\n",
    "df_train['Embarked'].fillna(df_train['Embarked'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1181d470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode 'Sex' column\n",
    "le = LabelEncoder()\n",
    "df_train['Sex'] = le.fit_transform(df_train['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bc8d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode 'Embarked' column\n",
    "df_train['Embarked'] = le.fit_transform(df_train['Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53278342-207b-4261-881f-938267fa1a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0a3180-c32a-42dd-9efb-d66e2b7e4057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot boxplot and histogram\n",
    "def plot_boxplot_hist(data, column):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 8))\n",
    "\n",
    "    # Box plot\n",
    "    sns.boxplot(x=data[column], color='salmon', ax=axes[0])\n",
    "    axes[0].set_title(f'Box Plot of {column}')\n",
    "    axes[0].set_xlabel('')\n",
    "\n",
    "    # Histogram\n",
    "    sns.histplot(data[column], kde=True, color='dodgerblue', ax=axes[1])\n",
    "    axes[1].set_title(f'Histogram of {column}')\n",
    "    axes[1].set_xlabel(column)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot box plot and histogram for each numeric column one by one\n",
    "for col in df_train.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    plot_boxplot_hist(df_train, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d745732a-33fc-4e62-8f41-d8933831220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers of present\n",
    "\n",
    "Q1 = df_train.quantile(0.25)\n",
    "print(f'Q1:\\n{Q1}')\n",
    "Q3 = df_train.quantile(0.75)\n",
    "print(f'\\nQ3:\\n{Q3}')\n",
    "IQR = Q3 - Q1\n",
    "print(f'\\nIQR:\\n{IQR}')\n",
    "\n",
    "# Calculate lower and upper bounds\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "print(f'\\nLower Bound:\\n{lower_bound}')\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "print(f'\\nUpper Bound:\\n{upper_bound}\\n')\n",
    "\n",
    "# Outliers\n",
    "outliers = {}\n",
    "for col in df_train.columns:\n",
    "    df_train_outliers = df_train[(df_train[col] < lower_bound[col]) | (df_train[col] > upper_bound[col])]\n",
    "    if not df_train_outliers.empty:\n",
    "        outliers[col] = df_train_outliers[col]\n",
    "        print(f\"Outlier(s) in column '{col}':\")\n",
    "        print(df_train_outliers)\n",
    "    else:\n",
    "        print(f\"No outliers in column '{col}'\")\n",
    "\n",
    "# Remove outliers\n",
    "df_train_no_outliers = df_train[(df_train >= lower_bound) & (df_train <= upper_bound)].dropna().reset_index(drop=True)\n",
    "print(\"\\nNo outliers:\")\n",
    "print(df_train_no_outliers)\n",
    "\n",
    "# Re-assigning to new variable\n",
    "df_train = df_train_no_outliers\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc812a1-6b80-4a8b-967a-56bb0435b01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot boxplot and histogram\n",
    "def plot_boxplot_hist(data, column):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 8))\n",
    "\n",
    "    # Box plot\n",
    "    sns.boxplot(x=data[column], color='salmon', ax=axes[0])\n",
    "    axes[0].set_title(f'Box Plot of {column}')\n",
    "    axes[0].set_xlabel('')\n",
    "\n",
    "    # Histogram\n",
    "    sns.histplot(data[column], kde=True, color='dodgerblue', ax=axes[1])\n",
    "    axes[1].set_title(f'Histogram of {column}')\n",
    "    axes[1].set_xlabel(column)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot box plot and histogram for each numeric column one by one\n",
    "for col in df_train.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    plot_boxplot_hist(df_train, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830546f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the correlation matrix as a heatmap\n",
    "fig= plt.figure(figsize=(18, 8))\n",
    "sns.heatmap(df_train.corr(), annot=True);\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2533c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Titanic_test.csv\n",
    "df_test = pd.read_csv('Titanic_test.csv')\n",
    "df_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74430003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total no.of rows and columns\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba2b1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop serier which aren't necessary\n",
    "df_test = df_test.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120316cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the training set\n",
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1a98fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "df_test['Age'].fillna(df_test['Age'].median(), inplace=True)\n",
    "\n",
    "df_test['Fare'].fillna(df_test['Fare'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7013e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode 'Sex' column\n",
    "le = LabelEncoder()\n",
    "df_test['Sex'] = le.fit_transform(df_test['Sex'])\n",
    "\n",
    "# Encode 'Embarked' column\n",
    "df_test['Embarked'] = le.fit_transform(df_test['Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c522f215",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee236e5-77ef-4d98-9968-68aefec8e255",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to plot boxplot and histogram\n",
    "def plot_boxplot_hist(data, column):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 8))\n",
    "\n",
    "    # Box plot\n",
    "    sns.boxplot(x=data[column], color='salmon', ax=axes[0])\n",
    "    axes[0].set_title(f'Box Plot of {column}')\n",
    "    axes[0].set_xlabel('')\n",
    "\n",
    "    # Histogram\n",
    "    sns.histplot(data[column], kde=True, color='dodgerblue', ax=axes[1])\n",
    "    axes[1].set_title(f'Histogram of {column}')\n",
    "    axes[1].set_xlabel(column)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot box plot and histogram for each numeric column one by one\n",
    "for col in df_test.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    plot_boxplot_hist(df_test, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee445031-884e-46d7-996d-1f4f7cef8be9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove outliers of present\n",
    "\n",
    "Q1 = df_test.quantile(0.25)\n",
    "print(f'Q1:\\n{Q1}')\n",
    "Q3 = df_test.quantile(0.75)\n",
    "print(f'\\nQ3:\\n{Q3}')\n",
    "IQR = Q3 - Q1\n",
    "print(f'\\nIQR:\\n{IQR}')\n",
    "\n",
    "# Calculate lower and upper bounds\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "print(f'\\nLower Bound:\\n{lower_bound}')\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "print(f'\\nUpper Bound:\\n{upper_bound}\\n')\n",
    "\n",
    "# Outliers\n",
    "outliers = {}\n",
    "for col in df_test.columns:\n",
    "    df_test_outliers = df_test[(df_test[col] < lower_bound[col]) | (df_test[col] > upper_bound[col])]\n",
    "    if not df_test_outliers.empty:\n",
    "        outliers[col] = df_test_outliers[col]\n",
    "        print(f\"Outlier(s) in column '{col}':\")\n",
    "        print(df_test_outliers)\n",
    "    else:\n",
    "        print(f\"No outliers in column '{col}'\")\n",
    "\n",
    "# Remove outliers\n",
    "df_test_no_outliers = df_test[(df_test >= lower_bound) & (df_test <= upper_bound)].dropna().reset_index(drop=True)\n",
    "print(\"\\nNo outliers:\")\n",
    "print(df_test_no_outliers)\n",
    "\n",
    "# Re-assigning to new variable\n",
    "df_test = df_test_no_outliers\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9a10cd-733b-4a57-a870-14fed71744a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to plot boxplot and histogram\n",
    "def plot_boxplot_hist(data, column):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 8))\n",
    "\n",
    "    # Box plot\n",
    "    sns.boxplot(x=data[column], color='salmon', ax=axes[0])\n",
    "    axes[0].set_title(f'Box Plot of {column}')\n",
    "    axes[0].set_xlabel('')\n",
    "\n",
    "    # Histogram\n",
    "    sns.histplot(data[column], kde=True, color='dodgerblue', ax=axes[1])\n",
    "    axes[1].set_title(f'Histogram of {column}')\n",
    "    axes[1].set_xlabel(column)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot box plot and histogram for each numeric column one by one\n",
    "for col in df_test.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    plot_boxplot_hist(df_test, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14d466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(18, 8))\n",
    "sns.heatmap(df_test.corr(), annot=True);\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4c3dff-0a2e-4d78-a251-2398e3397cbf",
   "metadata": {},
   "source": [
    "## Build a Logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47f3f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and target variable\n",
    "X_train = df_train[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
    "y_train = df_train['Survived']\n",
    "\n",
    "X_test = df_test[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ef1f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dd83d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the logistic regression model\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d082764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the training set\n",
    "y_train_pred = log_reg.predict(X_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test = log_reg.predict(X_test)\n",
    "\n",
    "# Predict the target values for df_test to create y_test\n",
    "y_test_proba = log_reg.predict_proba(X_test)[:, 1]  # Get the probabilities for the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7ed01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_test_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e45c71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the training set\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(f'Training Accuracy: {train_accuracy}')\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08a69db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for the training set\n",
    "cm = confusion_matrix(y_train, y_train_pred)\n",
    "print(\"Confusion Matric\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d217024-939a-405f-ae12-1349cf148d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the intercept\n",
    "intercept = log_reg.intercept_[0]\n",
    "\n",
    "# Display the coefficients\n",
    "print(f'Intercept: {intercept}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd6ffdb-053e-4ec0-8c1c-d8d77363ad38",
   "metadata": {},
   "source": [
    "#### Overall Performance:\n",
    "\n",
    "With an overall accuracy of 80%, the model performs reasonably well on the training set. However, the performance can be further evaluated and potentially improved, especially for class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da62a3a-4432-4e8a-83e8-8f91590368ac",
   "metadata": {},
   "source": [
    "#### Next Steps\n",
    "1. Check for Class Imbalance:\n",
    "    - Investigate if there is a class imbalance and consider techniques like resampling, SMOTE, or adjusting class weights to address it.\n",
    "2. Evaluate on Test Set:\n",
    "    - Assess the model's performance on the test set to ensure it generalizes well and is not overfitting the training data.\n",
    "3. Model Tuning:\n",
    "    - Experiment with different algorithms, hyperparameters, or feature engineering to improve overall performance and, specifically, the performance for class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d45b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting the decision boundary (for 2D visualization, we reduce the dimensions using PCA or t-SNE)\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "# X_train_pca = pca.fit_transform(X_train)\n",
    "# X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# # Train the model on the reduced data\n",
    "# log_reg.fit(X_train_pca, y_train)\n",
    "\n",
    "# # Visualize the decision boundaries for the training set\n",
    "# X_set, y_set = X_train_pca, y_train\n",
    "# X1, X2 = np.meshgrid(np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.01),\n",
    "#                      np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.01))\n",
    "# plt.contourf(X1, X2, log_reg.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "#              alpha=0.75, cmap=ListedColormap(['#FF9999', '#1E90FF']))\n",
    "# plt.xlim(X1.min(), X1.max())\n",
    "# plt.ylim(X2.min(), X2.max())\n",
    "# for i, j in enumerate(np.unique(y_set)):\n",
    "#     plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "#                 c=ListedColormap(['#FF9999', '#1E90FF'])(i), label=j)\n",
    "# plt.title('Logistic Regression (Training set)')\n",
    "# plt.xlabel('PCA Feature 1')\n",
    "# plt.ylabel('PCA Feature 2')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08d641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize the decision boundaries for the test set\n",
    "# X_set, y_set = X_test_pca, y_test\n",
    "# X1, X2 = np.meshgrid(np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.01),\n",
    "#                      np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.01))\n",
    "# plt.contourf(X1, X2, log_reg.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "#              alpha=0.75, cmap=ListedColormap(['#FF9999', '#1E90FF']))\n",
    "# plt.xlim(X1.min(), X1.max())\n",
    "# plt.ylim(X2.min(), X2.max())\n",
    "# for i, j in enumerate(np.unique(y_set)):\n",
    "#     plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "#                 c=ListedColormap(['#FF9999', '#1E90FF'])(i), label=j)\n",
    "# plt.title('Logistic Regression (Test set)')\n",
    "# plt.xlabel('PCA Feature 1')\n",
    "# plt.ylabel('PCA Feature 2')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5739ed0b-78f4-4ed4-a9ad-6d0dd1edc625",
   "metadata": {},
   "source": [
    "### Discussing the Significance of Features in Predicting the Target Variable (Survival Probability)\n",
    "\n",
    "In any predictive modeling task, understanding the significance of features is crucial because it helps in interpreting the model's decisions and improving its performance. In the context of predicting survival probability (as in the Titanic dataset), each feature contributes differently to the model's ability to make accurate predictions. Hereâ€™s a discussion on the significance of features without delving into Python code:\n",
    "\n",
    "#### 1. Feature Importance\n",
    "Feature importance refers to techniques that assign a score to each feature based on how useful they are at predicting the target variable. In the case of survival prediction, we want to know which factors most strongly influence whether a passenger survived or not.\n",
    "\n",
    "#### 2. Common Significant Features in Survival Prediction\n",
    "\n",
    "- Passenger Class (Pclass):\n",
    "  - Social-economic status represented by the passenger class is a significant predictor. Historically, higher class passengers had better access to lifeboats and safety measures, increasing their chances of survival.\n",
    "\n",
    "- Sex:\n",
    "  - Gender plays a crucial role in survival probability. The principle of \"women and children first\" meant that women had a higher likelihood of being rescued.\n",
    "\n",
    "- Age:\n",
    "  - Age is another important feature. Children were given priority for lifeboats, so younger passengers often had higher survival rates. Elderly passengers might have had lower survival rates due to physical limitations during the evacuation.\n",
    "\n",
    "- Fare:\n",
    "  - The fare paid for the ticket can correlate with passenger class and economic status. Higher fares often indicate higher class and better accommodations, potentially leading to a higher chance of survival.\n",
    "\n",
    "- SibSp (Number of Siblings/Spouses aboard the Titanic):\n",
    "  - Having family members onboard might influence survival chances. Families tended to stay together, and those with family members might have had a better chance of securing spots on lifeboats.\n",
    "\n",
    "- Parch (Number of Parents/Children aboard the Titanic):\n",
    "  - Similar to SibSp, this feature indicates family ties, which can affect survival. Passengers traveling with children might have been given priority in rescue efforts.\n",
    "\n",
    "- Embarked (Port of Embarkation):\n",
    "  - The port where a passenger boarded might have some correlation with survival, potentially due to differences in nationality, socio-economic status, or cabin location.\n",
    "\n",
    "#### 3. Interaction Effects\n",
    "Features might not act independently; interactions between features can also be significant. For example, the survival probability might be particularly high for female passengers in first class or for young children with their parents. Understanding these interactions can provide deeper insights into the factors affecting survival.\n",
    "\n",
    "#### 4. Model Interpretation\n",
    "Interpreting the significance of features helps in multiple ways:\n",
    "- Improving Model Accuracy: By focusing on the most significant features, you can simplify the model and reduce overfitting.\n",
    "- Enhancing Explainability: It makes the model's predictions more interpretable, which is crucial for stakeholders who need to understand the reasoning behind predictions.\n",
    "- Guiding Feature Engineering: Knowing which features are significant can guide you in creating new features or transforming existing ones to boost model performance.\n",
    "\n",
    "#### 5. Real-world Implications\n",
    "Understanding feature significance isn't just about model accuracy. In the context of the Titanic, it provides historical insights into the factors that influenced survival during the disaster. It helps us understand human behavior, safety protocols, and societal norms of the time.\n",
    "\n",
    "By carefully analyzing and interpreting the significance of features, we can build more accurate, robust, and interpretable models that not only predict outcomes effectively but also provide valuable insights into the underlying data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291b2559-6c0b-4261-83d1-0419b56f0c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(log_reg, 'logistic_regression_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
